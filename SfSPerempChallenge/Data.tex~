\chapter{Data} \label{c:data}

Without data, performing an analysis that incorporated more than the history and legal argumentation presented in Chapter
\ref{c:background} is impossible. This proved problematic. While the motivation of this text was a Canadian case, no comprehensive
Canadian data sets which exmained jury selection in Canada could be found. The increased prominence of the jury selection process
in the United States garnered a more fruitful search.

The author is heavily indebted to \citeauthor{JurySunshineProj}; \citeauthor{StubbornLegacy}; and
\citeauthor{PerempChalMurder}. These authors shared their data freely with the author, providing him with a wealth of data to
analyse empirically. As a consequence of the multiple separate data sets, however, care must be taken to describe each of the data
sets separately in order to capture adequately the different methodologies and sources they represent. Critically, it should be
noted that each of these papers represents effort on the part of the authors. As \cite{JurySunshineProj} notes:

\begin{quote}
  \centering
   limited public access to court data reinforces the single-case focus of the legal doctrines related to jury selection. Poor
   access to records is the single largest reason why jury selection cannot break out of the litigato's framework to become a
   normal topic for political debate
\end{quote}

Currently, the collection of jury data is difficult, as many courtrooms have not digitized past records and concerns over privacy
limit the release of those records, which are stored as paper documents in the case file (see \cite{JurySunshineProject}). This
limits the ability of an individual to ask for summaries across numerous trials or to view the jury selection process on a scale
beyond the basis of one case. Thus, to gather aggregate data the authors of these papers necessarily used different collection
techniques dictated by the scope of collection desired and the procedures of the court systems from which data was collected.

\section{Jury Sunshine Project} \label{sec:jspdata}

The Jury Sunshine Project (\cite{JurySunshineProj}), so named as it was carried out in order to shed light on the jury
selection process, is the most extensive data set which was provided to the author. It endeavoured to collect jury data for all
felony trial cases in North Carolina in the year 2011, which ultimately resulted in a data set that detailed the simple
demographic characteristics and trial information of 29,624 individuals summoned for jury duty in 1,306 trials. Note that not all
entries were complete.

Due to the scope of the project, there are a number of problems which had to be solved by the authors. The first of these was
simply identifying which court cases went to trial in 2011, in order to direct resources effectively. This was accomplished by
downloading publicly available case data from the North Carolina Administrative Office of the Courts (NCAOC)\footnote{The link provided in
  the Jury Sunshine Paper to the specific source (http://www.nccourts.org/Citizens/SRPlanning/Statistics/CAReports\_fy16-17.asp)
  does not appear to be working as of January 2019, however the NCAOC seems to provide an API functionality at
  https://data.nccourts.gov/api/v1/console/datasets/1.0/search/} and determining the case numbers and counties of cases which went
to trial. \citeauthor{JurySunshineProj} state that this likely missed some cases which went to trial, but that they were
confident that a ``strong majority'' of trials was collected, which did not systematically differ from those excluded.

This list was then used to perform a pilot study to refine recording practices before undertaking a more general survey where
``law students, law librarians, and undergraduate students'' (called \textit{collectors} for convenience) visited court clerk
offices to collect the relevant case data, including the presiding judge, prosecutor, defence lawyer, defendant, venire members,
charges, verdict, and sentence. The case files also included data about whether a venire member was removed by cause or
peremptorily, and the party which challenged in the peremptory case. Using public voter databases, bar admission records, and
judge appointment records, these collectors were able to determine demographic (race, gender, and date of birth) and political
affiliation data for the venire members, lawyers, defendants, and judges. This data set was stored stored in a relational database
provided to the author by Dr. Ronald Wright.

The analysis of the data provided in \cite{JurySunshineProj} was limited to aggregate summaries of the trends at the venire
member level. That is to say, they examined the strike trends for both the defence and the prosecution, conditioning on some
additional variables. There was also spatial analysis performed, were different urban counties were directly compared. These
analyses were not statistical in nature, and were displayed using contingency tables.

\section{Stubborn Legacy Data} \label{sec:norcardata}

\cite{StubbornLegacy} also provided data to the author, albeit a more limited set. This study, also based in North Carolina,
focused on the trials of inmates on death row as of July 1, 2010, yielding a total of 173 cases. In each proceeding, the study
examined only those venire members not excluded for cause, and critically the analysis of the study focused only on prosecutorial
peremptory challenges. Besides collecting demographic data as in the Jury Sunshine Case, this study also collected attitudinal
data for the venire members.

Staff attorneys from the Michigan State University College of Law were responsible for the data collection in this study. The work
was performed similarly to the Jury Sunshine Data, using case files to collect information about the court proceedings such as the
peremptory challenges used, presiding judge, prosecutor, and defence lawyer. Detailed verdict and charge information was not
collected, as the preselection criteria of death row inmates made the verdict clear, and the death penalty can only be applied for
certain crimes.

To collect demographic and attitudinal data, the juror questionnaire sheets were consulted\footnote{As \cite{StubbornLegacy}
  observe, self-identified race may be the most accurate source of racial group identification}. These sheets are typically used
as a component of voir dire, in order to make the process more efficient and determine venire members categorically ineligible for
jury duty. As a result, they inquire about opinions on the death penalty, for example, as well as demographic questions. As not
all jury questionnaires were available, additional information was collected from jury roll lists to determine the races of the
final jury members. It should be noted that this collection was done blind and to high standards of proof, and a reliability study
carried out in \cite{StubbornLegacy} indicated that under this system the race coding was 97.9\% accurate when the standards were
met. Those for whom the standards were not met were marked as ``Unknown.''

The analysis performed in this paper was more statistical than in the Jury Sunshine Data. Contingency tables generated using the
data were tested using Chi-squared independence tests, and a simple logistic regression model was created to predict prosecutorial
strikes. One minor criticism which could be made of their methodology is the lack of a consistent level to their tests. It seems
that rather than class these tests as significant or not, these tests were simply performed to report the p-values they
returned. Additionally, there are possible multiple testing issues as the study seems to indicate multiple tests were performed on
each table, with the specific test used to generate the reported p-values not clearly indicated.

\section{Philadelphia Data} \label{sec:phillydata}

\cite{PerempChalMurder} presents the most comprehensively analyzed data of the three data sets.

\section{Data Cleaning} \label{sec:datacleaning}

\subsection{Sunshine Data}

\subsubsection{Flattening the Data}

For greater expediency of analysis, the relational database of the Jury Sunshine Data was first flattened. The relational database
was read into Microsoft Excel and the \texttt{readxl} package (\cite{readxl}) was used to read the excel file into the programming
language \Rp. A wrapper for the \texttt{merge} function was developed which provided simple a simple output detailing failed
matches in an outer join in order to ensure that the flattening of the data into a matrix did not miss important data due to
partial incompleteness. The code for this wrapper can be seen in \ref{app:proccode}.

This wrapper revealed only a small number of irregularities in the data, which are detailed in \ref{app:irregs}:

\begin{enumerate}
\item Twenty-nine charges missing trial information such as the presiding judge (all of trials with IDs of the form 710-0XX)
\item Twenty-six prosecutors not associated with any trials and missing demographic data
\item One trial missing charge information
\end{enumerate}

Ultimately, the jurors for trial ID 710-01, the trial missing a charge from above, were included in the data as their records were
complete otherwise. The prosecutors and charges which could not be joined were excluded from any future analysis, as they could
have easily been included by collectors by accident. Due to the small relative size of these inconsistencies relative to the size
of the data set, they did not cause concern.

\subsubsection{Preprocessing}

Of course there were other irregularities in the data than the obvious ones that arose in the flattening process.

The data collected in North Carolina proved invaluable to this project \cite{JurySunshineProj}.

\underline{Problem}: some columns of the data contained only NA values
\underline{Solution}: \texttt{lapply} to remove these uninformative columns

\underline{Problem}: relational database provided did not have all data in one joined table
\underline{Solution}: creation of \texttt{CleaningMerge} function: a wrapper for \texttt{merge} which provides information about the
mismatches which may be present in the two merged tables

\underline{Problem}: inconsistently coded levels, e.g. inconsistent case or ``?'' instead of ``U'' for unknowns
\underline{Solution}: forcing levels to be uppercase and the replacement of obvious mis-specified levels

\underline{Problem}: some columns seem to have swapped values, e.g. the gender column should be one of ``M'', ``F'', or ``U'' and the
political affiliation column should be one of ``D'', ``R'', ``I'', or ``U'', but some individuals have the gender recorded as
``R'' and political affiliation as ``M''
\underline{Solution}: the creation of the \texttt{IdentifySwap} function, which has two arguments: a data set and the acceptable or correct
levels for the variables in the data set. It then identifies rows which have candidate swaps and presents them for review
