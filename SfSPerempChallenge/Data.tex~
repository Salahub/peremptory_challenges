\chapter{Data} \label{c:data}

Without data, performing an analysis that incorporated more than the history and legal argumentation presented in Chapter
\ref{c:background} is impossible. This proved problematic. While the motivation of this text was a Canadian case, no comprehensive
Canadian data sets which exmained jury selection in Canada could be found. The increased prominence of the jury selection process
in the United States garnered a more fruitful search.

The author is heavily indebted to \citeauthor{JurySunshineProj}; \citeauthor{StubbornLegacy}; and
\citeauthor{PerempChalMurder}. These authors shared their data freely with the author, providing him with a wealth of data to
analyse empirically. As a consequence of the multiple separate data sets, however, care must be taken to describe each of the data
sets separately in order to capture adequately the different methodologies and sources they represent. Critically, it should be
noted that each of these papers represents effort on the part of the authors. As \cite{JurySunshineProj} notes:

\begin{quote}
   limited public access to court data reinforces the single-case focus of the legal doctrines related to jury selection. Poor
   access to records is the single largest reason why jury selection cannot break out of the litigato's framework to become a
   normal topic for political debate
\end{quote}

Currently, the collection of jury data is difficult, as many courtrooms have not digitized past records and concerns over privacy
limit the release of those records, which are stored as paper documents in the case file (see \cite{JurySunshineProj}). This
limits the ability of an individual to ask for summaries across numerous trials or to view the jury selection process on a scale
beyond the basis of one case. Thus, to gather aggregate data the authors of these papers necessarily used different collection
techniques dictated by the scope of collection desired and the procedures of the court systems from which data was collected.

\section{Jury Sunshine Project} \label{sec:jspdata}

\subsection{Methodology}

The Jury Sunshine Project (\cite{JurySunshineProj}), so named as it was carried out in order to shed light on the jury
selection process, is the most extensive data set which was provided to the author. It endeavoured to collect jury data for all
felony trial cases in North Carolina in the year 2011, which ultimately resulted in a data set that detailed the simple
demographic characteristics and trial information of 29,624 individuals summoned for jury duty in 1,306 trials. Note that not all
entries were complete.

Due to the scope of the project, there are a number of problems which had to be solved by the authors. The first of these was
simply identifying which court cases went to trial in 2011, in order to direct resources effectively. This was accomplished by
downloading publicly available case data from the North Carolina Administrative Office of the Courts (NCAOC)\footnote{The link provided in
  the Jury Sunshine Paper to the specific source (http://www.nccourts.org/Citizens/SRPlanning/Statistics/CAReports\_fy16-17.asp)
  does not appear to be working as of January 2019, however the NCAOC seems to provide an API functionality at
  https://data.nccourts.gov/api/v1/console/datasets/1.0/search/} and determining the case numbers and counties of cases which went
to trial. \citeauthor{JurySunshineProj} state that this likely missed some cases which went to trial, but that they were
confident that a ``strong majority'' of trials was collected, which did not systematically differ from those excluded.

This list was then used to perform a pilot study to refine recording practices before undertaking a more general survey where
``law students, law librarians, and undergraduate students'' (called \textit{collectors} for convenience) visited court clerk
offices to collect the relevant case data, including the presiding judge, prosecutor, defence lawyer, defendant, venire members,
charges, verdict, and sentence. The case files also included data about whether a venire member was removed by cause or
peremptorily, and the party which challenged in the peremptory case. Using public voter databases, bar admission records, and
judge appointment records, these collectors were able to determine demographic (race, gender, and date of birth) and political
affiliation data for the venire members, lawyers, defendants, and judges. This data set was stored stored in a relational database
provided to the author by Dr. Ronald Wright.

The analysis of the data provided in \cite{JurySunshineProj} was limited to aggregate summaries of the trends at the venire
member level. That is to say, they examined the strike trends for both the defence and the prosecution, conditioning on some
additional variables. There was also spatial analysis performed, were different urban counties were directly compared. These
analyses were not statistical in nature, and were displayed using contingency tables. Regardless the stark differences between
prosecution and defence with regards to race were a key finding when the aggregate data was analyzed.

\subsection{Cleaning}

\subsubsection{Flattening the Data}

For greater expediency of analysis, the relational database of the Jury Sunshine Data was first flattened. The relational database
was read into Microsoft Excel and the \texttt{readxl} package (\cite{readxl}) was used to read the excel file into the programming
language \Rp. A wrapper for the \texttt{merge} function was developed which provided simple a simple output detailing failed
matches in an outer join in order to ensure that the flattening of the data into a matrix did not miss important data due to
partial incompleteness. The code for this wrapper can be seen in \ref{app:proccode}.

This wrapper revealed only a small number of irregularities in the data, which are detailed in \ref{app:irregs}:

\begin{enumerate}
\item Twenty-nine charges missing trial information such as the presiding judge (all of trials with IDs of the form 710-0XX)
\item Twenty-six prosecutors not associated with any trials and missing demographic data
\item One trial missing charge information
\end{enumerate}

Ultimately, the jurors for trial ID 710-01, the trial missing a charge from above, were included in the data as their records were
complete otherwise. The prosecutors and charges which could not be joined were excluded from any future analysis, as they could
have easily been included by collectors by accident. Due to the small relative size of these inconsistencies relative to the size
of the data set, they did not cause concern.

\subsubsection{Uninformative Columns}

Of course there were other irregularities in the data than the obvious ones that arose in the flattening process. There were a
handful of likely sources for these errors. The first of these is the anonymization of the data for public use. The private data
includes a wealth of privileged data such as juror name and address, and these were removed in the data given to the
author.

As a consequence of this anonymization as well as the inclusion of rarely used columns such as those for additional notes, some
columns of the data contained only NA values. Most baffling of these was the \texttt{BirthDate} variable in the \texttt{Jurors}
table, as there was no clear reason for this data to be missing. Thankfully, none of the missing columns were relevant to the
joins performed in flattening, and they would have been only secondary in data analysis. As a consequence, these uninformative
columns were simply removed from the data.

\subsubsection{Coding Inconsistencies}

Related to this problem was the issue of inconsistently coded variable levels. An example of these inconsistencies would be levels
recorded as both lower and upper case letters, or the presence of ``?'' instead of ``U'' for unknown values. It is very likely
this inconsistency was a direct result of the data collection method which used many data collectors working independently in
different places at different times. Thankfully, \citeauthor{JurySunshineProj} provided the codebook used by data collectors,
which served as the authoritative reference for the admissible factor levels of demographic variables. Solving this problem was as
simple as setting all demographic variable levels to be uppercase and replacing obviously mis-specified levels.

One specific inconsistency which should be noted is that of the outcome, which had a handful of entries recorded as ``HC'', an
inadmissible level not defined by the codebook. It is quite possible that this level represented a typo, as the ``H'' and ``G''
keys are adjacent on the American QWERTY keyboard layout, and ``GC'' was the code for 'guilty as charged', but out of a desire to
be conservative with the data the occurrence of this outcome was replaced with U instead. Additionally, the inadmissible level
``G'' was replaced by GC.

\subsubsection{Swaps}

A more difficult problem of level misspecification was the presence of what appeared to be columns with swapped values, frequently
occurring with the gender column (the admissible levels of which are ``M'', ``F'', and ``U'') and the political affiliation column
(the admissible levels of which are ``D'', ``R'', ``I'', or ``U''). The aformentioned ``swaps'' appeared as records in which, for
example, the gender was recorded as ``R'' and political affiliation as ``M''. More complicated swaps of three columns also
occurred. To address this problem, the \texttt{IdentifySwap} function was written (see line 108 in \ref{app:proccode}).

The \texttt{IdentifySwap} function accepts two arguments: a data frame with named columns and a named list of vectors of the
acceptable levels for some of the column names. It then performs vectorized checks of the specified column names and presents any
rows which may have swaps or errors interactively to the user, along with a suggested reorder to ``un-swap'' the row. The user can
press enter to accept the suggested reordering, enter some other reordering, or enter 0 to indicate that the row was not a true
swap, but simply an error. The un-swapped entries are then returned to the data, and the rows with errors have the erroneous
values replaced by ``U'', the universal code for ``Unknown'' within all data variables\footnote{One notable exception to this
  insertion of ``Unknown'' was the case of the judge Arnold O Jones II, whose gender was not recorded in the data, but who was
  identifiable as a man using a quick Google search of his unique name}.

The source of these swaps is also most likely the data collection method. The codebook provided specifically notes that the data
collection was meant to record the race (R), gender (G), and political affiliation (P) data in the form RGP, but it is not
inconceivable that it would occasionally have been recorded or entered in some other ordering in the tedium of data entry. In any
case, this problem affected only 431 records of the nearly 30,000, suggesting that the recorded error rate was not unacceptably
large.

\subsubsection{Charge Classification}

Perhaps the least regular data in this data set was that of the charge text. Due to the lack of any codebook guidance about the
standard way of recording a charge in a trial, identical charges were recorded in numerous ways. The first method used to combat
this was removing non-alphnumeric characters, extra spaces, and converting all charges to lower case. This still left a
considerable variation, however. Consider the charge of breaking and entering, for example, even with this simple preprocessing
performed the entries varied significantly (e.g. ``break or enter'', ``breaking andor entering'', ``breaking and or entering'',
etc.).

As a consequence, the processing was more involved. First the most common versions of the charge text for the charges were all
regularized to be identical (see \texttt{StringReg} in \ref{app:proccode}. Next, a regular expression classification tree was
developed, which would also account for specific features of a charge. When identifying murder, for example, it seemed important
to ensure attempted murder was separated from murder itself, and separating first and second degree was also desired. This tree
would, when presented with a charge, apply the regular expressions at each node to the charge. If the charge matched the
expression at a node, the regular expressions of that node's children were applied to the charge until it was classified to some
leaf node, each of which had a standardized value which replaced the charge. A small example of this structure is displayed in
Figure \ref{fig:exampletree}, and the full tree is visualized in \ref{app:complement} in Figure \ref{fig:chargetree}.

\begin{figure}[!h]
  \centering
  \epsfCfile{0.6}{SimpleSubTree}
  \caption[Simple Charge Tree Example]{A example of a simple charge classification tree to separate the sexual offenses from
    charges leveled against previously known sex offenders. A charge would be classified from most general on the left to most
    specific on the right.}
  \label{fig:exampletree}
\end{figure}

By performing regularization using this charge tree, regularized charges were guaranteed. The cost of this regularization was the
inability to classify all crimes, however. Of the 1407 charges present in the data, the tree provides regularization for
1209. With additional time and inspection of the failed matches, the tree could conceivably be axpanded to regularize all
charges. As this was not the primary investigation of the report, however, such effort was not expended.

Instead, a number of helpful aggregation and extraction functions were developed to allow for the charges to be further
simplified, as seen in \ref{app:proccode}. In this report, they were aggregated by intuitive classes: sex-based offenses, thefts,
murders, drug charges, violent offenses not otherwise classified, and driving charges. However, other classes, such as the North
Carolina felony classes themselves (as provided by \cite{offenseclass}), may provide a more informative classification rationale.

\subsubsection{Variable Level Renaming}

The final step of the data cleaning process was to convert the uninformative codes used to indicate variable values to more
intuitive and clear names (for example to convert ``I'' in the political affiliation variable to ``Ind'', a clearer indication of
independent). Certain variables which were already clear, such as gender (codes ``M'', ``F'', ``U''), were not renamed due to the
clarity of the one letter representations.

\subsection{Variable Synthesis}

In order to expand the possible analysis and visualization potential, a number of variables were synthesized from the Jury
Sunshine data set. They are detailed below.

\begin{description}
  \item[Race Match] A logical variable which is true for a venire member if they are the same race as the defendant, and false
    otherwise. This variable was motivated in particular by the Gerald Stanley trial, in which the implicit contention of those
    who have taken issue with peremptory challenge is that the First Nations venire members were removed by the defence as their
    race did not match that of Stanley in the racially charged trial.
  \item[Guilty] Logical indicator indicating whether the trial outcome was guilty or not
  \item[Visible Minority] Logical indicator of non-white venire member race
  \item[Race of Striking Party] Factor variable which gives the race of the prosecution if the venire member was struck by the
    prosecution, the race of the defence if the venire member was struck by the defence
  \item[Simplified Race] Due to the scarcity of the other minority races, this variable simplified the race provided to white,
    black, or other for the venire member
  \item[Simplified Defendant Race] The same as the simplified race for the defendant races
\end{description}

\section{Stubborn Legacy Data} \label{sec:norcardata}

\subsection{Methodology}

\cite{StubbornLegacy} also provided data to the author, albeit a more limited set. This study, also based in North Carolina,
focused on the trials of inmates on death row as of July 1, 2010, yielding a total of 173 cases. In each proceeding, the study
examined only those venire members not excluded for cause, and critically the analysis of the study focused only on prosecutorial
peremptory challenges. Besides collecting demographic data as in the Jury Sunshine Case, this study also collected attitudinal
data for the venire members.

Staff attorneys from the Michigan State University College of Law were responsible for the data collection in this study. The work
was performed similarly to the Jury Sunshine Data, using case files to collect information about the court proceedings such as the
peremptory challenges used, presiding judge, prosecutor, and defence lawyer. Detailed verdict and charge information was not
collected, as the preselection criteria of death row inmates made the verdict clear, and the death penalty can only be applied for
certain crimes.

To collect demographic and attitudinal data, the juror questionnaire sheets were consulted\footnote{As \cite{StubbornLegacy}
  observe, self-identified race may be the most accurate source of racial group identification}. These sheets are typically used
as a component of voir dire, in order to make the process more efficient and determine venire members categorically ineligible for
jury duty. As a result, they inquire about opinions on the death penalty, for example, as well as demographic questions. As not
all jury questionnaires were available, additional information was collected from jury roll lists to determine the races of the
final jury members. It should be noted that this collection was done blind and to high standards of proof, and a reliability study
carried out in \cite{StubbornLegacy} indicated that under this system the race coding was 97.9\% accurate when the standards were
met. Those for whom the standards were not met were marked as ``Unknown.''

The analysis performed in this paper was more statistical than in the Jury Sunshine Data. Contingency tables generated using the
data were tested using Chi-squared independence tests, and a simple logistic regression model was created to predict prosecutorial
strikes. One minor criticism which could be made of their methodology is the lack of a consistent level to their tests. It seems
that rather than class these tests as significant or not, these tests were simply performed to report the p-values they
returned. Additionally, there are possible multiple testing issues as the study seems to indicate multiple tests were performed on
each table, with the specific test used to generate the reported p-values not clearly indicated.

\subsection{Cleaning}

\section{Philadelphia Data} \label{sec:phillydata}

\subsection{Methodology}

\cite{PerempChalMurder} presents a similar data set collected using similar means. Court files such as the juror questionnaire,
voter registration, and census data were all used to complete juror demographic information for 317 venires consisting of 14,532
venire members in Philadelphia capital murder cases between 1981 and 1997\footnote{This study took into account the sampling error
  by reweighting venires based on the  year of the trial and the defendant race, as court records showed that the sample coverage
  varied over these factors}. It should be noted that this data included only those jurors kept or peremptorily struck, venire
members struck for cause were not included in the data. The procedure used to determine race using the census and voter registration polls was quite
complicated, but was rigorously performed using accepted census methods to a standard of 98\% reliability\footnote{Additionally,
  imputation was only performed in a small minority of cases}.

In their incredibly thorough analysis of the data, there were findings consistent with both the Jury Sunshine and Stubborn Legacy
data. The defence and prosecution seem to follow mirrored patterns of racial preference in the use of peremptory challenges, even
when controlling for other possible confouding effects.

\subsection{Cleaning}

One interesting quirk of the Philadelphia data set was missing values. While the codebook describing the data explicitly stated a
number of variables should be recorded as binary values. In the provided data files, however, these variables were missing for a
majority of the observations. In the case of the ``FINLJURY'' variable for example, an indicator of whether the jury member was
included in the final jury, there were 4626 records with a value of 1, 3 with a value of 0, and 12890 missing values. These
missing values were assumed to be zero, as using this assumption created a data set which was consistent with that reported in
\cite{PerempChalMurder}.